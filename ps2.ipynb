{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6255337a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfcd40a4a6d9da75b47efd231cb2e043",
     "grade": true,
     "grade_id": "cell-2020906bd78d79d5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "\n",
    "Group nr: 22\n",
    "\n",
    "Name 1 and CID: Rasmus Bäckström (RASBACKS)\n",
    "\n",
    "Name 2 and CID: Gabriella Bengtsson (GABBENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dbb017ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa26dfc2f35f895dabff1310042d379f",
     "grade": false,
     "grade_id": "cell-14bb5073a539cbd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mining_world import Environment\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64aea2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6903b1a761cb59cbd2251b2033afc820",
     "grade": false,
     "grade_id": "cell-7aa6c85811a632c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This assignment dels with data structures, K-NN, tree-based classifiers and evaluation of the methods deployed on a fictional scenario.  \n",
    "\n",
    "## Mining world \n",
    "\n",
    "<img src=\"imgs/poster.png\" width=\"800\"/>\n",
    "\n",
    "## Scenario\n",
    "\n",
    "\n",
    "Humanity has now reached a point where we need to extract and refine more Copium, a precious resource with great value. The only problem is that Copium can only be found on certain uninhabitable planets. This of course means that automated robots are sent instead.      \n",
    "\n",
    "Copium is naturally very unstable and is only exists very temporary before it decays. There are very specific geological activities and circumstances needed for copium to form. The life cycle of Copium follows. First, a hot stream of liquid magma flows to the surface, creating a hotspot that that looks like a small crater. At the surface, if the conditions are correct, copium can form during the cool-down period. But as stated previously, Copium is unstable in its natural environment and decays to other materials shortly after. \n",
    "\n",
    "The formation of these deposits craters are very random, but the heat from them can easilly be detected with a satellite. But there is no way of knowing if the newly formed depoist contains copium from just a satellite, therefor there is a robot rover on the ground with sensors that can collect further measurements. The rover has many sensors that can measure the properties of the ground below it, but of course, Copium can not directly be detected with these types of sensors. This is where the machine learning approach will be used, to take all those measurements and try to classify if the deposit contains Copium or not.  \n",
    "\n",
    "\n",
    "<img src=\"imgs/overView.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad74f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14ffb7e00ae02f2be65dafa3854c8b57",
     "grade": false,
     "grade_id": "cell-80e56d230a18f7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Navigation - Tree search\n",
    "\n",
    "This section will show how the naivigation is done. This is not a part of the assignment to understand, but will be used. \n",
    "\n",
    "##  Breadth first\n",
    "\n",
    "The method used is a breadth first search algorithm, it is one of the simplest tree search algorithms and basically tries every option for a fixed number of steps and chooses the best one. The robot only have 4 actions, it can move at every time step in a chosen direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606296fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "128f22d73852ea6a5372822b0b37c784",
     "grade": false,
     "grade_id": "cell-c992a6248dd856e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Actions \n",
    "\n",
    "<img src=\"imgs/actions.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e5627207",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32d128b3906d7686ac730d0aca4908af",
     "grade": false,
     "grade_id": "cell-9a8f005a81419a11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, actor):\n",
    "        self.actor = actor\n",
    "        self.total_score = 0\n",
    "    \n",
    "    def update(self, action, inherited_score):\n",
    "        score = self.actor.step(action)\n",
    "        self.total_score = 1.05*inherited_score + score\n",
    "        return self.total_score\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1afda62f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb47522e94f5a7e21138374b9cca9f85",
     "grade": false,
     "grade_id": "cell-7e1a22235cf88444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def breadth_first_search(actor, max_depth, action_space):\n",
    "    node = Node(copy.deepcopy(actor)) \n",
    "    queue_keys = ['0'] # queue to keep track of nodes that has not yet been expanded.  \n",
    "    visited = {queue_keys[0]: node} # saves visited nodes in order to not recalulate the entire path for each step. \n",
    "    \n",
    "    max_score = -np.inf\n",
    "    best_action = None\n",
    "\n",
    "    while True:\n",
    "        key = queue_keys.pop(0)\n",
    "        if len(key) > max_depth: # stop at a set depth \n",
    "            break    \n",
    "        node = visited[key]\n",
    "        \n",
    "        for action in action_space: # expand all children nodes\n",
    "            child_node = copy.deepcopy(node)  # copy current node\n",
    "            score = child_node.update(action=action, inherited_score=node.get_score()) # update node with action\n",
    "            child_key = key + action # create child node key\n",
    "            \n",
    "            if score > max_score: # save best path \n",
    "                max_score = score\n",
    "                best_action = child_key[1]\n",
    "                \n",
    "            visited[child_key] = child_node  # add child node to visited nodes.\n",
    "            queue_keys.append(child_key)  # add child node queue of non expanded nodes. \n",
    "            \n",
    "    return best_action ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68fa06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f80922e17efb57f64f6dbbfd7c7225d9",
     "grade": false,
     "grade_id": "cell-bc5397658a16ae14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Collect data\n",
    "\n",
    "The first step is to collect data that will be used for both training and validation. The available types features can be seen with env.get_sensor_properties() and the actual measurements can be retrieved with env.get_sensor_readings(). It will return a dictionary with the same keys as in env.get_sensor_properties() containg a value for each feature. If the robot is not currently over a deposit, then it will return None. The true label can be extracted with env.get_ground_truth(), which will return a 1 if there is copium in the deposit and 0 if not. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5586ce9b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4599469bdfc1083c8909d4445d94330a",
     "grade": true,
     "grade_id": "cell-564b8441165efae4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Have True for first time running and then set to false. \n",
    "collect_data = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8a864720",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "731bc4a9f7eefb5692544b4896a2e748",
     "grade": false,
     "grade_id": "cell-e5ae97d8d9debdab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor properties ['ground_density', 'moist', 'reflectivity', 'silicon_rate', 'oxygen_rate', 'iron_rate', 'aluminium_rate', 'magnesium_rate', 'undetectable']\n"
     ]
    }
   ],
   "source": [
    "env = Environment(map_type=1, fps=500, resolution=(1000, 1000))\n",
    "env.exit()\n",
    "sensor_properties = env.get_sensor_properties()\n",
    "print('Sensor properties', sensor_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "77dc7249",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e494e8fce0535af86d13d035a8e10603",
     "grade": false,
     "grade_id": "cell-060d3a2c29bfe32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if collect_data == True:\n",
    "    env = Environment(map_type=1, fps=500, resolution=(1000, 1000))\n",
    "    sensor_properties = env.get_sensor_properties()\n",
    "\n",
    "    # We can initilize the dictionary the following way.\n",
    "    data = dict()\n",
    "    data['copium'] = [] \n",
    "    for key in sensor_properties:\n",
    "        data[key] = []\n",
    "\n",
    "    for i in range(5000):\n",
    "        action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "        env.step(action)\n",
    "        # if we are over a deposit. \n",
    "        if env.get_sensor_readings() is not None:\n",
    "            sensor_readings = env.get_sensor_readings()\n",
    "            copium = env.get_ground_truth()\n",
    "\n",
    "            for key in sensor_readings:\n",
    "                data[key].append(sensor_readings[key])\n",
    "            data['copium'].append(copium)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    env.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96abd44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf20cd270c452764c144604d39f9d328",
     "grade": false,
     "grade_id": "cell-5d9009d13bc1d8f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exerscie 1: Data structure\n",
    "\n",
    "## a) Pandas data frame \n",
    "In this assignment we will work pandas data frame for storing the collected data. First create a pandas data frame from the dictionary. The documentation for it can be found at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html, only the data feild needs to be filled in with the created dictionary. Call this data frame df. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "40ae59e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18afbbd66a35181cac0d3f8d649981b8",
     "grade": false,
     "grade_id": "cell-733f079ee3c0ac51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: create pandas dataframe\n",
    "\n",
    "if collect_data == True:\n",
    "    df = pd.DataFrame(data= data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341255d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9a1e3412453720dd6456a9477872245",
     "grade": false,
     "grade_id": "cell-79c4c3bbd4567c1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "## b) Save data\n",
    "Collecting data for each time the notebook is opened is time consuming and the tests are not as reproducable if the data is different each time. Therefore we save the dateframe and we can load the data instead. After this you can set collect_data=False above. Use the function pandas function df.to_csv('name_of_file.csv', index=False)\n",
    "\n",
    "The task is to save the data in a file named mydata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a22f5dd0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c9b2f586a4f5a8444748e5322d74957",
     "grade": true,
     "grade_id": "cell-ff6790fec729ba0a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if collect_data == True:\n",
    "    # TODO: save data to a csv file named mydata.  \n",
    "    df.to_csv(\"mydata.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ceccb6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f28baa70f888bbd347e4ed2cd06a79f",
     "grade": false,
     "grade_id": "cell-387be287dd96e402",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) Load data\n",
    "The task is to load the data from mydata.csv into a dataframe named df. Use the function pd.read_csv('name_of_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0c0c91dd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e370fd9485b66347115ffc5e77c0a29d",
     "grade": true,
     "grade_id": "cell-9db787fbf7a3f989",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: read data from mydata.csv into a dataframe called df.  \n",
    "df = pd.read_csv(\"mydata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6ff64c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620b5082820a7c26abfda4edec2c4eb3",
     "grade": false,
     "grade_id": "cell-ebfce4b848cdc9f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# From the data frame you can access all data for a key with for example:\n",
    "#print(\"All data for a feature \\n\", df[\"copium\"])\n",
    "#print(df[\"ground_density\"])\n",
    "\n",
    "# You can access a single sample with:\n",
    "\n",
    "#print(\"Single sample from index \\n\", len(df.iloc[1]))\n",
    "\n",
    "# You can access all freatures but one with:\n",
    "#all_features_without_copium = df.drop(columns='copium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7a9bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70f5b5f338fbfd7148f2f4d1073bbd2b",
     "grade": false,
     "grade_id": "cell-90bd2839fd455f3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) Part 1: Split data\n",
    "\n",
    "Here we will devide the data into a training set and a test set. Good rule of thumb is to use 80% of the data in the training set and 20 % in the test set. The the two data sets should be randomly sampled (shuffle=True), we also set he seed with random_state so that the split is reproducable. This is done with train_test_split() from sklearn, https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. The syntax looks like \n",
    "\n",
    "train, test = train_test_split(dataframe, test_size=ratio_test_set, shuffle=True, random_state=42)\n",
    "\n",
    "Why is it important that the data is shuffled when it is split, what could happen otherwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45397d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53bce3ebb6b20c538748a546b7fd13c5",
     "grade": true,
     "grade_id": "cell-e9dac1f1c72ed455",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer: Otherwise we could have a bias. Because they can have been grouped or made in order before. So we have to shuffle it to make sure we dont get that bias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "18e21475",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65406c33cc5ec7553e81ad195c39129b",
     "grade": true,
     "grade_id": "cell-d3da8450794a3f7d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: devide the data into train and test set. \n",
    "train,test = train_test_split(df, test_size = 0.2 , shuffle = True, random_state = 42)\n",
    "#print(test[\"copium\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35d89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32af0aaf1e8fa024b7091f469b51ac11",
     "grade": false,
     "grade_id": "cell-df82c101fb36b17b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Part 1: Analyse data balance\n",
    "\n",
    "The occurance can be retrevied with .value_counts() from a pandas date frame. Here get the occurance of copium in the samples. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "04c7e1f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91bac4c7e7622383262c58b7e7a257df",
     "grade": true,
     "grade_id": "cell-7c51441927cdeb22",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copium\n",
      "0    687\n",
      "1    123\n",
      "Name: count, dtype: int64\n",
      "copium\n",
      "0    174\n",
      "1     29\n",
      "Name: count, dtype: int64\n",
      "copium\n",
      "0    861\n",
      "1    152\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of samples with copium and the number of samples without copium.\n",
    "training_copium = train.value_counts(\"copium\")\n",
    "print(training_copium)\n",
    "test_copium = test.value_counts(\"copium\")\n",
    "print(test_copium)\n",
    "total_copium = df.value_counts(\"copium\")\n",
    "print(total_copium)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeccc46",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec667850e64bd69400083d01a46ed070",
     "grade": true,
     "grade_id": "cell-49be72b9d48244e2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer: The number of samples with copium is a lot less than the samples without, therefore the dataset is not balanced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801c409",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76f3b50c8e1733fc0206b99f3581c09",
     "grade": false,
     "grade_id": "cell-116683159a32e20b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Part 2: Balance data\n",
    "\n",
    "We will throughout this assignment test each method with the original dataset and a balanced dataset. We show how balanceing can be done by downsampling below. Your task is to create an upsampled balanced dataset. You only need to use the upsampled data set for the rest of the other part 2) exerices. \n",
    "\n",
    "What could be the reason for chosing either upsampled or downsampeled metohd to balance the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f727f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5f76e04d0efc78cb1d663dd6823e930",
     "grade": true,
     "grade_id": "cell-605878f4821edafc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer: \n",
    "We would like to upsample if the data we want is a small minority, if we upsamlpe with extra copies/variations of the minority you can make the model learn faster and at the same time balance the distribution. But with a risk of overfitting because of duplicates. We downsample if we feel that the minority has enough data to make it reliable while training. If we have a large dataset, downsampling can be good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3437d2f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7c0a9612378d3f6f0f67d2146a4a071",
     "grade": false,
     "grade_id": "cell-63de30dce9039bf1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO balance data det. \n",
    "# Seperate the data into something that contains copium and one that doesn't,\n",
    "# can for example be done with df[df[\"copium\"]==0] etc.\n",
    "train_zero = train[train[\"copium\"]==0]\n",
    "train_one = train[train[\"copium\"]==1]\n",
    "\n",
    "# downsample majority\n",
    "train_zero_downsampled = resample(train_zero,\n",
    "                               n_samples=train_one.shape[0])\n",
    "\n",
    "train_balanced_downsampled = pd.concat([train_one, train_zero_downsampled])\n",
    "down_copium = train_balanced_downsampled.value_counts(\"copium\")\n",
    "#print(down_copium)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: upsample minority \n",
    "train_one_upsampled = resample(train_one, n_samples = train_zero.shape[0] )\n",
    "\n",
    "train_balanced_upsampled = pd.concat([train_zero, train_one_upsampled])\n",
    "up_copium = train_balanced_upsampled.value_counts(\"copium\")\n",
    "\n",
    "\n",
    "#print(train_zero) 687 rows\n",
    "#print(train_balanced_upsampled) #810 rows\n",
    "#print(train_balanced_upsampled[\"copium\"][0:len(train_balanced_upsampled)/2])\n",
    "\n",
    "\n",
    "#train_balanced_upsampled = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad551eda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a87a456265dd5534908ee9dd0a749378",
     "grade": true,
     "grade_id": "cell-af2ba3b41b0e32f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1d537",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61b1cd3925a3265c2c4bbcada0fb0d1d",
     "grade": false,
     "grade_id": "cell-073e09c001bc6760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: Performance evaluation\n",
    "\n",
    "Here we will define a class that later will be used for evaluation the performance of the classification methods. It will be used to log the accuracy, precsision and recalll. More information about precision and recall can be found at https://en.wikipedia.org/wiki/Precision_and_recall. \n",
    "\n",
    "Expliain why the different metics are usefull and why is not always accuarcy enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a91be1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ffd749262a2e421ca7d3f10a7360587",
     "grade": true,
     "grade_id": "cell-c17f13f39f1c121d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\n",
    "These all are there to understand a models performance. Precision measure the accuracy of positive predictions and thus it ratio, it focuses on the reliability of positive predictions. Recall is also known as sensitivity and measures the models ability to identity, it identify all positive instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3904d3a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec699f7a00c5cc3a60a67f482992e418",
     "grade": false,
     "grade_id": "cell-30d4bb59031d814a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Classification_eval(object):\n",
    "    def __init__(self):\n",
    "        # counters \n",
    "        self.TP = 0 # correctly identified positive \n",
    "        self.FP = 0 # falsely identified positive \n",
    "        self.TN = 0 # correctly identified negative \n",
    "        self.FN = 0 # falsely identified negative \n",
    "    \n",
    "    def update(self, pred, label):\n",
    "        \"\"\"\n",
    "        pred - is the prediction will be either a 1 or 0. \n",
    "        label - is the correct answer, will be either a 1 or 0.\n",
    "        \"\"\"\n",
    "        # TODO: add to one of the counters each time this function is called.\n",
    "        if (pred == 1) and (label == 1):\n",
    "            self.TP += 1\n",
    "        elif  (pred == 0) and (label == 0):\n",
    "            self.TN += 1\n",
    "        elif (pred == 1) and (label == 0):\n",
    "            self.FN += 1\n",
    "        else:\n",
    "            self.FP += 1\n",
    "       \n",
    "\n",
    "    def accuracy(self): \n",
    "        # returns the accuracy \n",
    "        if (self.TP + self.TN) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the accuracy.\n",
    "        if (self.TP + self.TN + self.FN + self.FP) == 0:\n",
    "            return 0\n",
    "        \n",
    "        accuracy = (self.TP + self.TN)/(self.TP + self.TN + self.FN + self.FP)\n",
    "\n",
    "        return np.round(accuracy, 4)\n",
    "    \n",
    "    def precision(self): # percentage of the estimated positive that actually is positive\n",
    "        if (self.TP + self.FP) == 0:\n",
    "            return 0\n",
    "        precision = self.TP/(self.TP + self.FN)    \n",
    "        #precision = self.TP/(self.TP + self.FP)  #Faktiska formeln enligt föreläsning#\n",
    "        \n",
    "        # TODO: calculate the precision.\n",
    "        return np.round(precision, 4)\n",
    "    \n",
    "    def recall(self): # percentage of correctly identified positive of the total positive\n",
    "        if (self.TP + self.FN) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the recall.\n",
    "        \n",
    "        recall = self.TP/(self.TP + self.FP)\n",
    "        #recall = self.TP/(self.TP + self.FN) #Faktiska formeln enligt föreläsning#\n",
    "        return np.round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ef15a4bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4993a82e0fdc175fba56ddf7af2f381",
     "grade": true,
     "grade_id": "cell-9f928939ef661cd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n"
     ]
    }
   ],
   "source": [
    "# Test Classification_eval\n",
    "\n",
    "test_eval = Classification_eval()\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(0,1)\n",
    "test_eval.update(1,0)\n",
    "test_eval.update(1,0)\n",
    "test_eval.update(0,0)\n",
    "test_eval.update(0,0)\n",
    "test_eval_ = np.allclose([test_eval.accuracy(), test_eval.precision(), test_eval.recall()], [0.625, 0.6, 0.75])\n",
    "assert test_eval_\n",
    "if test_eval_:\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02359150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc9f9d30243f35da0eb921d1ed9df9d6",
     "grade": false,
     "grade_id": "cell-42e7fff982a76017",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 4: K- nearest neighbours\n",
    "\n",
    "## a) Normalize\n",
    "\n",
    "Here we will code our K-NN classifier, method 2.1 on page 21 in the book has the psudo code for K-NN. We will start with the data normalization, i.e. we will normalize the input data so that each feature has the same range in terms of max/min values. The min value can be found with data.min(), similarly for the max value. \n",
    "\n",
    "Why is it important that the data is normalized for the K-NN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c067ca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30acb00aaff6fd9caefa4e560880300a",
     "grade": true,
     "grade_id": "cell-96776ce3a5e4ac7d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer: We need to normalize in order to not neglect smaller distances while larger distances dominate.\n",
    "You want to normalizide thus the KNN is distancedepending and if you dont normalizide the extreme values that can effect the distancecalculations. Through normalizing you make the algorithm more secure against these and make the datapoints to approximate same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "31958a4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a3b2b21a5d0cb1463a24b5d74c1b440",
     "grade": false,
     "grade_id": "cell-1965136e57f66697",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        # normalize the data and return it. \n",
    "       \n",
    "        return (data-self.min)/(self.max-self.min)\n",
    "    \n",
    "    def update_normalization(self, data):\n",
    "        # Save the min and max values for each feature. This funciton is only used for the training data.\n",
    "        self.min = data.min()\n",
    "        self.max = data.max()\n",
    "        return self.normalize(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d34a6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "    def __init__(self, k):\n",
    "        self.features = None # normalized features from training data \n",
    "        self.labels = None # the corresponding labels (if there is copium)\n",
    "        self.normalize = Normalize() # class instance for normalization\n",
    "        self.k = k # the k value in k-nn algorithm.\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, features, labels):\n",
    "\n",
    "        normalized_features = self.normalize.update_normalization(features)\n",
    "        features = normalized_features\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        #normailize\n",
    "        sample = self.normalize.normalize(sample)\n",
    "        #lists to hold k nearest points\n",
    "        list_dist = np.ones(self.k)*np.inf\n",
    "        list_prediction = np.zeros(self.k)\n",
    "        #find k nearest points\n",
    "        for i in range(self.features.shape[0]):\n",
    "            dist = np.linalg.norm(self.features.iloc[i]-sample)\n",
    "            if dist < np.max(list_dist):\n",
    "                idx = np.argmax(list_dist)\n",
    "                list_dist[idx] = dist\n",
    "                list_prediction[idx] = self.labels.iloc[i]\n",
    "        prediction = int(np.round(np.mean(list_prediction)))\n",
    "                \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9ce32e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4cccdb1ba0f9041ddc6b26f75a2a88a",
     "grade": true,
     "grade_id": "cell-f560e22fdec3f7de",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n"
     ]
    }
   ],
   "source": [
    "# Test the KNN against known values. The test only covers small sample and there are hidden tests which will \n",
    "# tested after submitting. \n",
    "\n",
    "test_train = pd.read_csv('test.csv')\n",
    "test_test = pd.read_csv('test2.csv')\n",
    "\n",
    "train_labels = test_train['copium']\n",
    "train_features = test_train.drop(columns='copium')\n",
    "test_labels = test_test['copium']\n",
    "test_features = test_test.drop(columns='copium')\n",
    "\n",
    "knn = KNN(k=5)\n",
    "knn.fit(train_features, train_labels)\n",
    "pred_list = []\n",
    "\n",
    "for i in range(test_features.shape[0]):\n",
    "    pred = knn.predict(test_features.iloc[i])\n",
    "    pred_list.append(pred)\n",
    "val = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "if np.allclose(pred_list, val):\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    \n",
    "#print((pred_list),\"hej\", (val)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9ba18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b998718e3379c65266541338c61c43f9",
     "grade": false,
     "grade_id": "cell-dde11302217e4fe7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) part 1: Evaluate the K-NN \n",
    "Evaluate the K-NN and choose a suitable k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "68f2a5c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b789d4ce7c7ba911a2bac909cdcef21f",
     "grade": true,
     "grade_id": "cell-6bd1e79f38dda753",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy 0.867\n",
      "Precision 0.5263\n",
      "Recall 0.6897\n"
     ]
    }
   ],
   "source": [
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "train_labels = train_balanced_upsampled['copium']\n",
    "train_features = train_balanced_upsampled.drop(columns='copium')\n",
    "\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "\n",
    "# TODO, try differenent values of k. \n",
    "knn = KNN(k=50)\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "log = Classification_eval()\n",
    "for i in range(x.shape[0]):\n",
    "    pred = knn.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "\n",
    "print('Accuarcy', log.accuracy())\n",
    "print('Precision', log.precision())\n",
    "print('Recall', log.recall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545be3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e5e669568b89ae0d76cc4c8fa32f982",
     "grade": true,
     "grade_id": "cell-16eeb836b8b6e78b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Try some differnet values of k and just looking at these resutlts would the klassifier work well for all k? \n",
    "\n",
    "Answer:\n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 |0.867 | 0.5556| 0.3448|  \n",
    "| 5 |0.7783 | 0.2759| 0.25|   \n",
    "| 20 | 0.8621| 0| 0|   \n",
    "| 50 |0.8571 |0 | 0|  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40aa1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71e823aa49829ee8019fa0faa47356f9",
     "grade": true,
     "grade_id": "cell-62d238ce7dd4e123",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## c) part 2, balanced data. \n",
    "Now try with the upsampled balanced data, test for the same k values as in part 1. Are the results different and whitch one would identify more copium?  \n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.867| 0.5556 |0.3448 |  \n",
    "| 5 |0.8227 |0.4186 | 0.6207|   \n",
    "| 20 |0.9064 | 0.6316| 0.8276|   \n",
    "| 50 | 0.9015| 0.6286| 0.7586|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89904a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2ebdb355c2900e1ccbdddef700e9d07",
     "grade": false,
     "grade_id": "cell-9970ec54e795a716",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) kNN with Sklearn\n",
    "Typically one would not implement these algorithms by hand for each time you need them. Here we show how the kNN algorithm from sklearn can be used. Note, it does not normalize the features by default. The advantanges with the sklearn version is in computational cost as they utilize clever tree structures to minimize the search problem. \n",
    "\n",
    "A quick note: Even if ready packages exist, it is still very valuable to understand the principels behind the algorithms. For example when the learning doesn't work as well as you liked (happens more often then not...), you can reason to why and modify the problem or method into something that does work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2beeb5b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "974914833f9157cbd7879805a66d2af0",
     "grade": false,
     "grade_id": "cell-bafc1dac9bf2fe6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 1  and from sklearn  [1]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n"
     ]
    }
   ],
   "source": [
    "# Set up data used for fitting the model\n",
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "# with the off the shelf model we need normalize the data before \n",
    "normalize = Normalize()\n",
    "data_features = normalize.update_normalization(train_features)\n",
    "\n",
    "# The test data\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "\n",
    "# Normalize the test data\n",
    "x_norm = normalize.normalize(x)\n",
    "\n",
    "# Set up an knn with sklearn \n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_sklearn.fit(data_features.values, train_labels.values)\n",
    "\n",
    "# Set up our own for comparison \n",
    "knn = KNN(k=5)\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# Test both for the 20 first values\n",
    "for i in range(20):\n",
    "    pred = knn.predict(x.iloc[i])\n",
    "    pred_sklearn = knn_sklearn.predict([x_norm.iloc[i]])\n",
    "    print('Prediciton from our model', pred, ' and from sklearn ',pred_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90f3e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d92f5d5dfaa4094e7cddec326d033f58",
     "grade": false,
     "grade_id": "cell-e0aa40fba01eeb67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 5: Tree based classifier\n",
    "\n",
    "In this exercise we will implement a thee based classifier. The implementation will be a class representing a node and will recursivly create child node until the entire tree structure is built. The general step for learning a tree structure from learning data is as follows:\n",
    "\n",
    "1. Initiate the the root node.\n",
    "2. Call the learn function with the learning data. \n",
    "\n",
    "Inside the learn function\n",
    "\n",
    "3. Check if all the data has the same classification, and check if the data set is smaller then min_node_size. If any of these are true, set the node as a leaf node and return. \n",
    "4. Find the spliting point in the data that returns the highest gini value, be careful to not use the label which we want to classify. \n",
    "5. Split the dataset, one is usually called head and the other tail. Save the splitting parameter and which value was used for the splitting. \n",
    "6. Check the prediction of the splited data sets (majority copium or not) and create two child nodes, one for each of the splited dataset. \n",
    "7. Call the the learn function of the child node with the corresponding split data and appdend the child node. After that return. \n",
    "\n",
    "The above will build the tree structure. To use the tree for prediction simply call the predict function of the root node with a data sample. In the predict function the following steps should happen.\n",
    "\n",
    "1. Check if the current node is a lead node, if it is simply return the self.prediction of that node. \n",
    "2. If it is not a leaf node then check the value of the splitting parameter in the data sample and compare with the splitting value. \n",
    "3. Call the predict function of the correspoinding child node with the data sample and return the answer.\n",
    "\n",
    "This will recurivly traverse the tree until a leaf node containing the prediction is reached. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e927e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9cbe3e399265962122affdf6fff4505",
     "grade": false,
     "grade_id": "cell-d05e3619e45ae5fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 a) Find split point\n",
    "\n",
    "First we define a function that can find the splitting point for a parameter with the highest gini value. \n",
    "\n",
    "The gini value can be described as:\n",
    "\n",
    "If $\\Gamma$ contains the set of all labeles, then $\\Gamma(x < c)$ would be all labels that belong to the criteria $x < c$, where $x$ is a feature or parameter and $c$ is the splitting criteia (split_value).  \n",
    "\n",
    "\n",
    "For example: \n",
    "\n",
    "If $x$ is iron_rate and our splitting critera is $c = 0.4$, then $\\Gamma(x < c)$ would be a set of all the label from samples with $\\textrm{iron_rate} < 0.4$.\n",
    "\n",
    "\n",
    "Then we can define:\n",
    "\n",
    "$v_1 = mean(\\Gamma(x < c))$\n",
    "\n",
    "$v_2 = mean(\\Gamma(x \\geq c))$\n",
    "\n",
    "$s_1 = v_1^2 + (1-v_1)^2$\n",
    "\n",
    "$s_2 = v_2^2 + (1-v_2)^2$\n",
    "\n",
    "We define $len(\\Gamma)$ to give the number of samples in the dataset, then the weighted gini value is:\n",
    "\n",
    "$s = \\frac{len(\\Gamma(x_i < c))}{len(\\Gamma)}*s_1 + \\frac{len(\\Gamma(x_i \\geq c))}{len(\\Gamma)}*s_2$\n",
    "\n",
    "The goal is to find a criteria $c$ that maximizes the gini value $s$. The easies is to order the data set after parameter, e.g. we could order all sample depending on how much iron_rate they have, then we can try to split the data between every data point in a for loop and save the one that has the highest gini value. The critieria c would be the average between two data points that are sorted, i.e. $c_i = \\frac{x_{i-1} + x_i}{2}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fa24f570",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b59c910799dff4462a727e393dc77935",
     "grade": false,
     "grade_id": "cell-caa92101f7dd232d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_split_point(data, label, parameter):\n",
    "    \"\"\"\n",
    "    data - all the data we want to split, our (gamma)\n",
    "    label - the parameter we want to classify. \n",
    "    parameter - the parameter we want to check for, our x_i\n",
    "    -----------\n",
    "    retrun:\n",
    "    split_value - the spliting value, our c. \n",
    "    gini_value - the gini value for the best c.\n",
    "    df_head - the data frame belonging to x_i < c\n",
    "    df_tail - the data frame belonging to x_i => c\n",
    "    \"\"\"\n",
    "    # Beging by sorting the data after the paramter. \n",
    "    sorted_data = data.sort_values(by=parameter)    #sorted by ground_density, ground_density is our x_i\n",
    "    sorted_label = sorted_data[label]\n",
    "    \n",
    "    # TODO loop through all the split points in the sorted data and find \n",
    "    # the best gini_value (s) and split_value (c).  \n",
    "    n = sorted_data.shape[0]\n",
    "    new_gini_value = 0\n",
    "    split_value = 0\n",
    "\n",
    "    for i in range(1,n):\n",
    "  \n",
    "        #kollar bara på y värden då x är på båda sidor av splitkravet\n",
    "        label_head = sorted_label[:i]      #can slice, but can't access single value for some reason\n",
    "        label_tail = sorted_label[i:]\n",
    "      \n",
    "        v1 = np.sum(label_head)/label_head.shape[0]\n",
    "        v2 = np.sum(label_tail)/label_tail.shape[0]\n",
    "   \n",
    "        s1 = v1 ** 2 +(1 - v1) ** 2\n",
    "        s2 = v2 ** 2 + (1 - v2) ** 2\n",
    "        \n",
    "        gini_value = label_head.shape[0]/n * s1 + label_tail.shape[0]/n * s2\n",
    "        if gini_value > new_gini_value:\n",
    "            new_gini_value = gini_value\n",
    "            split_value = (sorted_data[parameter].iloc[i-1] + sorted_data[parameter].iloc[i]) /2\n",
    "\n",
    "    df_head = sorted_data[sorted_data[parameter] < split_value]\n",
    "    df_tail = sorted_data[sorted_data[parameter] >= split_value]\n",
    "   \n",
    "    return split_value, new_gini_value, df_head, df_tail \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "accf1ba9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf3223c237aedc288036eee6c7e77676",
     "grade": true,
     "grade_id": "cell-cb0e81d04a850fb0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n"
     ]
    }
   ],
   "source": [
    "# Test for find_split_point function, the test does not test everything and more hidden tests are used \n",
    "# during grading. \n",
    "df_test = pd.read_csv('test.csv')\n",
    "split_value, gini_value, df_head, df_tail = find_split_point(df_test, 'copium', 'ground_density')\n",
    "if split_value == 1.8762144694722296 and gini_value == 0.7234487734487733:\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f30de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2223aa51cfa949a14d97613d8480f74",
     "grade": false,
     "grade_id": "cell-d311f53e9383d913",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 b) Tree Node\n",
    "\n",
    "Here we will code a tree node class that can recursivly call it self. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c1d660a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec3bd8704760a9fcaa3ed3b021b9b4d2",
     "grade": false,
     "grade_id": "cell-5d724b8a3a41de81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, prediction=None):\n",
    "        self.split_value = None # the splitting value (c)\n",
    "        self.split_parameter = None # what feature where uesd for the split (x_i)\n",
    "        self.child_nodes = [] # list that contains two child nodes, if not leaf_node\n",
    "        self.leaf_node = 0 # is this leaf_node (0=no, 1=yes)\n",
    "        self.prediction = prediction # classification made in this node.\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: To make a prediction we need to traverse the tree recursivly down to a leaf node and return \n",
    "        # the prediction. \n",
    "        # step 1: check if this is a leaf node, if it is then return prediction of this node, \n",
    "        if self.leaf_node == 1:\n",
    "            return self.prediction\n",
    "        # otherwise contine with step 2.\n",
    "        # step 2: Compare the input data with the splitting criteria.\n",
    "        if data[self.split_parameter] < self.split_value:\n",
    "            #self.child_nodes[left_childnode, right_child_node]\n",
    "            return self.child_nodes[0].predict(data)       #traversing recursively\n",
    "        else:\n",
    "            return self.child_nodes[1].predict(data)\n",
    "\n",
    "        # step 3: Call the prediction function in the corresponding child_node and return the prediction.\n",
    "         \n",
    "        \n",
    "            \n",
    "    def learn(self, data, label, min_node_size):\n",
    "        \"\"\"\n",
    "        data - the training data\n",
    "        label - the parameter we want to classify, i.e. \"copium\"\n",
    "        min_node_size - number of data points in a node for it to become a leaf node. \n",
    "        \"\"\"\n",
    "        # TODO: wirte the learning function.\n",
    "        new_split_value = 0 \n",
    "        new_gini_value = 0\n",
    "        new_df_head = 0\n",
    "        new_df_tail = 0\n",
    "      \n",
    "        # Step 1: check if this node should be turned into a leaf node, this happens if the data set is smaller \n",
    "        # then the min_node_size or if the data labels are homogenious. \n",
    "        if data[label].mean() == 1 or data[label].mean() == 0 or (len(data) < min_node_size):\n",
    "            self.leaf_node = 1\n",
    "            return\n",
    "            #return self.prediction\n",
    "        \n",
    "      \n",
    "  \n",
    "        \n",
    "    \n",
    "        \n",
    "        # Step 2: Loop over all features and get the best gini and split_value for each feature. \n",
    "        # Save the best split value and split_paramter and the two new data frames \n",
    "        # corresponding to the split [df_head, df_tail] (these will be data frames for the child nodes).  \n",
    "        #for feature in data.columns:\n",
    "        for feature in data.drop(columns=label).columns:\n",
    "           # print(feature)      #10 features\n",
    "            split_value1, gini_value, df_head, df_tail = find_split_point(data, label, feature) \n",
    "            if gini_value > new_gini_value:\n",
    "                new_gini_value = gini_value\n",
    "                new_split_value = split_value1\n",
    "                new_df_head = df_head   #dataframe for childnodes\n",
    "                new_df_tail = df_tail   #dataframe for childnodes\n",
    "                self.split_parameter = feature\n",
    "      \n",
    "       # self.child_nodes = [df_head, df_tail]\n",
    "        self.split_value = new_split_value\n",
    "       \n",
    "        majority_head = int(np.round(np.mean(new_df_head[label]))) #0\n",
    "        majority_tail = int(np.round(np.mean(new_df_tail[label]))) #1\n",
    "  \n",
    "        #print(int(df_tail))\n",
    "        #print(data.columns)\n",
    "      \n",
    "        child1 = TreeNode(prediction = majority_head)\n",
    "        child1.learn(new_df_head, label, min_node_size)\n",
    "        child2 = TreeNode(prediction = majority_tail)\n",
    "        child2.learn(new_df_tail, label, min_node_size)\n",
    "\n",
    "        self.child_nodes.append(child1)\n",
    "        self.child_nodes.append(child2)\n",
    "        #print(self.child_nodes)\n",
    "        \n",
    "        \n",
    "        # Step 3: Calculate the majority vote prediction of the two data frames. \n",
    "        # Create two child nodes, (e.g. child = TreeNode(prediction=1) for prediction copium) and and call the \n",
    "        # learn function with the corresponding dataframe. \n",
    "        \n",
    "        # Step 4: append the the child node to the self.child_nodes. It should be in the order \n",
    "        # of the child node correspoinding to [df_head, df_tail].\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "93629c10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bdabce60138486756f0bdee3dcc9c21",
     "grade": true,
     "grade_id": "cell-cfb150cc7e9ef0dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0] \n",
      " ! [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Test to give indicaion if the TreeNode class is correct.\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "tree = TreeNode() # create root node\n",
    "# learn the tree structure\n",
    "tree.learn(df_test, \"copium\", min_node_size=4)\n",
    "\n",
    "pred_list = []\n",
    "pred_answer = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "               0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, \n",
    "               0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
    "               1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    pred = tree.predict(df_test.iloc[i])\n",
    "    pred_list.append(pred)\n",
    "    \n",
    "if np.allclose(pred_list, pred_answer):\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "print(pred_list,\"\\n\", \"!\" ,pred_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbd608",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "329602f64e3ee9fb6de3d4c97197c96b",
     "grade": false,
     "grade_id": "cell-7949933930575680",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Train the Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d23eef95",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24e332b7422d03997e5a4c0b670f9ac1",
     "grade": true,
     "grade_id": "cell-d41091cb1d65774f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tree = TreeNode()\n",
    "tree.learn(train, \"copium\", min_node_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abee52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "484c482cce5a8adb2cdc9892b028d18d",
     "grade": false,
     "grade_id": "cell-959bd6f694aaadc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "db1c4812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy 0.8966\n",
      "precision 0.6429\n",
      "recall 0.6207\n"
     ]
    }
   ],
   "source": [
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "log = Classification_eval()\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    pred = tree.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "        \n",
    "print('accuarcy', log.accuracy())\n",
    "print('precision', log.precision())\n",
    "print('recall', log.recall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33168109",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc3f46bc5f9dfbaa5bd805bd23ac7be7",
     "grade": true,
     "grade_id": "cell-5ac1365691381dae",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## 5 c) part 1\n",
    "\n",
    "Use the non balanced data to learn the tree classifier and test the classifier for some different min_node_size values.\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1  | 0.8966 | 0.6429 | 0.6207 |  \n",
    "| 10 | 0.8916 | 0.6207 | 0.6207 |   \n",
    "| 20 | 0.8867 | 0.6364 | 0.4828 |   \n",
    "| 50 | 0.8818 | 0.6471 | 0.3793 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2713c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131c5207c1eeb8eb2d88e2e9042de27b",
     "grade": true,
     "grade_id": "cell-6a1e781f28a063e1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## 5 c) part 2, Try with balanced data\n",
    "\n",
    "Use the balanced data to learn the tree classifier and test the classifier for some different min_node_size values.\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1  |  0.8424| 0.4595 | 0.5862 |  \n",
    "| 10 | 0.8522 | 0.4857 | 0.5862 |   \n",
    "| 20 | 0.8571 | 0.5 | 0.5862 |   \n",
    "| 50 |0.8522  | 0.4865 | 0.6207 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2830e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c83dfd7cc7f5fd2c8432f54e91425f7c",
     "grade": false,
     "grade_id": "cell-4f6685679c5b4be9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) Tree classifier with sklearn \n",
    "\n",
    "Here we compare our tree classifier to the one implemented in sklearn. It is important to note that there can be differences between them, unlike our implementation the sklearn has some randomness in the traing and does not produce the same results each time. But overall they give simlar results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d6916f57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75e46887fd565771c52a1c1f578af180",
     "grade": false,
     "grade_id": "cell-6e90f7b38bbbe9f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [1] our tree pred  1\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [1] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [1] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [1] our tree pred  1\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n",
      "sklearn pred  [0] our tree pred  0\n"
     ]
    }
   ],
   "source": [
    "# Set up data used for fitting the model\n",
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "# Decition tree or classifier from sklearn \n",
    "tree_sklearn = DecisionTreeClassifier(min_samples_split=1.0, splitter='best', \n",
    "                             criterion='gini', min_samples_leaf=1)\n",
    "# Train the sklearn tree classifer\n",
    "tree_sklearn.fit(train_features.values, train_labels.values)\n",
    "\n",
    "# Train our tree classifier \n",
    "tree = TreeNode() # create root node\n",
    "tree.learn(train, \"copium\", min_node_size=1)\n",
    "\n",
    "# Test both and compare\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "# classify the data\n",
    "for i in range(30):\n",
    "    pred_sklearn = tree_sklearn.predict([x.iloc[i]])\n",
    "    pred = tree.predict(x.iloc[i])\n",
    "\n",
    "    print('sklearn pred ',pred_sklearn, 'our tree pred ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19598c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23d04d944edbe4b67aa9058bbb98b5ad",
     "grade": false,
     "grade_id": "cell-f59f64a8c41e294f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 6: Balance data analysis\n",
    "\n",
    "1. It there a difference between that results of the balanced and non-balanced data for the tree classifier?\n",
    "2. Is there a difference between the K-NN vs tree classifier?\n",
    "3. Which one would you expect to be a faster classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231f853",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23d8998f99e14c18babaa22afc7f2aa3",
     "grade": true,
     "grade_id": "cell-f67f58809daadcb6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Answer:\n",
    "1.  First of all, we should've swapped placed with recall and precision, we have them like this to pass the test.\n",
    "Yes. The upsampled is more consistent. We can see that the accuracy/precision and recall are less dependant on the min node size of our leaf nodes. In the non-balanced data we get less accuracy the larger we set our min node size to. \n",
    "\n",
    "2. The difference is that KNN gets better the more neighbours we are looking while upsampling, while if we decrease the min node size of our leaves in tree we get higher recall and precision.\n",
    "\n",
    "3. The tree classifier should be faster, especially scaling with larger data sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3840d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edce31490be1574477c5465dded60200",
     "grade": false,
     "grade_id": "cell-fa3691f04f48c770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 7: Deployment (Optional)\n",
    "\n",
    "Here we will try the learned classifiers on a larger map. Make sure that the last run version of K-NN and tree have good parameters i.e. k and min_node_size values. Which one found more copium?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b45b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aeb0fc0ae619e677ddedb5046c52706",
     "grade": true,
     "grade_id": "cell-c375db8a210a8a7d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = Environment(map_type=2, fps=5, resolution=(1000, 1000))\n",
    "\n",
    "try:\n",
    "    sensor_properties = env.get_sensor_properties()\n",
    "    sensor_sample = dict()\n",
    "    for key in sensor_properties:\n",
    "        sensor_sample[key] = [0]\n",
    "\n",
    "    log_knn = Classification_eval()\n",
    "    log_tree = Classification_eval()\n",
    "\n",
    "\n",
    "    for i in range(500):\n",
    "        action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "        env.step(action)\n",
    "        if env.get_sensor_readings() is not None:\n",
    "            sensor_readings = env.get_sensor_readings()\n",
    "            for key in sensor_readings:\n",
    "                sensor_sample[key][0] = sensor_readings[key]\n",
    "            sensor_sample_df = pd.DataFrame(sensor_sample)\n",
    "            log_knn.update(knn.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "            log_tree.update(tree.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "            env.plt_acc.update_acc(log_tree.accuracy(), log_knn.accuracy())\n",
    "        env.render()\n",
    "\n",
    "    env.exit()\n",
    "\n",
    "    print(\"K-NN accuracy \", log_knn.accuracy(), \"Tree accuracy\", log_tree.accuracy())\n",
    "    print(\"Number of copium deposits found, K-NN:\", log_knn.TP, \" Tree:\", log_tree.TP)\n",
    "except:\n",
    "    env.exit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf6c50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
